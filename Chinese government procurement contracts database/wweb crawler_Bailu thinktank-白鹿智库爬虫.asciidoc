+*In[ ]:*+
[source, ipython3]
----
import os
import sys
import datetime as dt
import time
import json
import re
import numpy as np
import pandas as pd
from urllib.request import quote
from selenium import webdriver   #比较适合动态网页的信息爬取
from selenium.webdriver.chrome.options import Options
import openpyxl


chrome_options = Options()
chrome_options.add_argument('--headless') #在做爬虫时，通常是不需要打开浏览器的，只需要使用浏览器的内核，因此可以使用Chrome的无头模式
chrome_options.add_argument('--disable-gpu') #禁用GPU加速
# 有形和无形
driver = webdriver.Chrome(options=chrome_options)
# driver = webdriver.Chrome()


driver.get('http://www.bailuzhiku.com/policy/list?keyword=%E6%99%BA%E6%85%A7%E5%9F%8E%E5%B8%82') 
time.sleep(2)

# #无限滚动的界面
# SCROLL_PAUSE_TIME = 0.5

# # Get scroll height
# last_height = driver.execute_script("return document.body.scrollHeight")

# while True:
#     # Scroll down to bottom
#     driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

#     # Wait to load page
#     time.sleep(SCROLL_PAUSE_TIME)

#     # Calculate new scroll height and compare with last scroll height
#     new_height = driver.execute_script("return document.body.scrollHeight")
#     if new_height == last_height:
#         break
#     last_height = new_height

results = []


for i in range(3):  #循环了三次，可能一次就够了，也可能不需要
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);") #滚动到页面底部
    time.sleep(2)

for i in range(1, 3):  #i=1-2
    obj = driver.find_element_by_xpath('//*[@id="NewsList"]/div[%d]/h3/a'%i)
    title = obj.get_attribute('title')
#     date = driver.find_element_by_xpath('//*[@id="NewsList"]/div[%d]/div[4]/ul/li[1]/span'%i).text
#     url = obj.get_attribute('href')
#     results.append([title, date, url])
    results.append(title)
    print(results)

print("爬取条数 %d" % len(results) )

# for ls in results:   #列举results中的所有元素
#     try:
#         driver.get(ls[2])
#         name = driver.find_element_by_xpath('//*[@id="barrierfree_container"]/div[3]/div[2]/div[2]/table/tbody/tr[1]/td[1]/span').text
#         danwei = driver.find_element_by_xpath('//*[@id="work_yy"]').text
#         counts = driver.find_element_by_xpath('//*[@id="barrierfree_container"]/div[3]/div[2]/div[2]/table/tbody/tr[4]/td[2]/span').text
#         industry = driver.find_element_by_xpath('//*[@id="barrierfree_container"]/div[3]/div[2]/div[2]/table/tbody/tr[2]/td[1]/span').text
#         APP = driver.find_element_by_xpath('//*[@id="barrierfree_container"]/div[3]/div[2]/div[2]/table/tbody/tr[3]/td[1]').text
#         time = driver.find_element_by_xpath('//*[@id="barrierfree_container"]/div[3]/div[2]/div[2]/table/tbody/tr[4]/td[1]').text
#         context = driver.find_element_by_xpath('//*[@id="content"]/div[26]"]').text
#         developer = driver.find_element_by_xpath('//*[@id="barrierfree_container"]/div[3]/div[2]/div[2]/table/tbody/tr[3]/td[2]/span').text
#         print(context)
#         time.sleep(1.5)
#     except:
#         pass

driver.quit()
----


+*In[ ]:*+
[source, ipython3]
----
import os
import sys
import datetime as dt
import time
import json
import re
import numpy as np
import pandas as pd
from urllib.request import quote
from selenium import webdriver
from selenium.webdriver.chrome.options import Options


chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')

driver = webdriver.Chrome(options=chrome_options)
driver = webdriver.Chrome()

results = []

driver.get('https://data.beijing.gov.cn/zyml/ajg/sswj/16765.htm')
time.sleep(10)

#设置cookie
cookies_dict = {}
cookies = driver.get_cookies()
for cookie in cookies:
    cookies_dict[cookie['name']] = cookie['value']


----


+*In[ ]:*+
[source, ipython3]
----
#登陆后搜索关键词
input = driver.find_element_by_xpath('//*[@id="txtSearch"]')#获取输入框
input.send_keys('智慧城市')#输入搜索关键词
driver.find_element_by_xpath('//*[@id="scrollTop"]/div[1]/div[2]/div[1]/div/div/div[2]/div[1]/div[1]/a').click()#点击搜索按钮
----


+*In[ ]:*+
[source, ipython3]
----
#获取相应的链接
for i in range(3):
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(30)

for i in range(100, 200):
    obj = driver.find_element_by_xpath('//*[@id="NewsList"]/div[%d]/h3/a' %i)
    title = ojb.get_attribute('title')
    link = obj.get_attribute('href')
    
driver.quit()

----


+*In[ ]:*+
[source, ipython3]
----


----
